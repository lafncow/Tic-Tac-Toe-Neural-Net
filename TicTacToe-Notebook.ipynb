{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "#raw_data = pd.read_csv(\"data_clean_lite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "# Create new game state\n",
    "def getNewGameState():\n",
    "    return np.zeros(9)\n",
    "\n",
    "# Choose Move\n",
    "## player = (-1|1)\n",
    "## game state = np.array((1,9))\n",
    "def getRandomMove(player, game_state):\n",
    "    \n",
    "    next_move = np.random.randint(9)\n",
    "    while not game_state[next_move] == 0:\n",
    "        next_move = (next_move + 1) % 9\n",
    "    #for k,v in enumerate(game_state):\n",
    "        #if game_state[k] == 0:\n",
    "            #next_move = k\n",
    "    return next_move\n",
    "\n",
    "# T|F if move is valid\n",
    "def validateMove(chosen_move, game_state):\n",
    "    return game_state[chosen_move] == 0\n",
    "\n",
    "# Get Updated game state\n",
    "def getUpdatedGameState(player, chosen_move, game_state):\n",
    "    new_game_state = np.copy(game_state)\n",
    "    new_game_state[chosen_move] = player\n",
    "    return new_game_state\n",
    "\n",
    "# Get Winning Player\n",
    "## -1 => player -1\n",
    "## 1 => player 1\n",
    "## 0 => tie\n",
    "## None => game is not over\n",
    "def getWinner(game_state):\n",
    "    winner = 0\n",
    "    for k,v in enumerate(game_state):\n",
    "        if game_state[k] == 0:\n",
    "            #board is not full, so is not a tie\n",
    "            winner = None\n",
    "            break\n",
    "        \n",
    "    winning_rows = [[0,1,2],[3,4,5],[6,7,8],[0,3,6],[1,4,7],[2,5,8],[0,4,8],[2,4,6]]\n",
    "    \n",
    "    for w in winning_rows:\n",
    "        if (np.sum(game_state[w]) == -3):\n",
    "            winner = -1\n",
    "        elif (np.sum(game_state[w]) == 3):\n",
    "            winner = 1\n",
    "    \n",
    "    return winner\n",
    "        \n",
    "def prettyGameStates(game_states):\n",
    "    pretty_states = np.reshape(game_states, (len(game_states),3,3))\n",
    "    return pretty_states\n",
    "\n",
    "def decayScores(n,p=1):\n",
    "\n",
    "    scores = np.linspace(0.5,1,len(n))*p\n",
    "    scores[len(scores)-1] += 3*p \n",
    "    return scores\n",
    "\n",
    "def mirrorStates(game_states):\n",
    "    num_states = len(game_states)\n",
    "    new_states = np.copy(game_states)\n",
    "    #Horizontal reflection\n",
    "    m_horiz = np.reshape(np.flip(prettyGameStates(game_states),axis=2),(num_states,9))\n",
    "    new_states = np.vstack([new_states, m_horiz])\n",
    "    #Flipped 90 degree rotation\n",
    "    new_states = np.vstack([new_states, np.reshape(np.transpose(prettyGameStates(game_states),axes=(0,2,1)),(num_states,9))])\n",
    "    #90 degree rotation\n",
    "    new_states = np.vstack([new_states, np.reshape(np.transpose(np.flip(prettyGameStates(game_states),axis=1),axes=(0,2,1)),(num_states,9))])\n",
    "    #180 degree rotation\n",
    "    new_states = np.vstack([new_states, np.flip(game_states,axis=1)])\n",
    "    #Flipped 180 degree rotation\n",
    "    new_states = np.vstack([new_states, np.reshape(np.flip(prettyGameStates(game_states),axis=1),(num_states,9))])\n",
    "    #Flipped 270 degree rotation\n",
    "    new_states = np.vstack([new_states, np.reshape(np.transpose(prettyGameStates(game_states),axes=(0,2,1)),(num_states,9))])\n",
    "    #270 degree rotation\n",
    "    new_states = np.vstack([new_states, np.reshape(np.transpose(prettyGameStates(m_horiz),axes=(0,2,1)),(num_states,9))])\n",
    "\n",
    "    return new_states\n",
    "\n",
    "def augmentData(game_states, scores):\n",
    "    #Create 7 mirror image games\n",
    "    mirrored_scores = np.tile(scores,8)\n",
    "    mirrored_states = mirrorStates(game_states)\n",
    "\n",
    "    #Create inverse games\n",
    "    #inverse_mirrored_states = mirrored_states * -1\n",
    "    #inverse_scores =  (mirrored_scores * -1) + 1\n",
    "\n",
    "    #new_game_states = np.vstack([mirrored_states, inverse_mirrored_states])\n",
    "    #new_scores = np.append(mirrored_scores, inverse_scores)\n",
    "    new_game_states = mirrored_states\n",
    "    new_scores = mirrored_scores\n",
    "    \n",
    "    return (new_game_states, new_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game\n",
    "\n",
    "def playGame(policy1=getRandomMove, policy2=getRandomMove):\n",
    "    if policy1 is None:\n",
    "        policy1 = getRandomMove\n",
    "    if policy2 is None:\n",
    "        policy2 = getRandomMove\n",
    "    policies = [policy1, policy2]\n",
    "    current_player = -1 #(-1|1)\n",
    "    # Start the game\n",
    "    game_state = getNewGameState()\n",
    "    # Create a log of game states\n",
    "    game_states = np.zeros((0,9))\n",
    "\n",
    "    turn_num = 0\n",
    "    winner = None\n",
    "    while turn_num < 10 and winner is None:\n",
    "        #Switch player\n",
    "        current_player = current_player * -1\n",
    "        #Pick a move\n",
    "        next_move = policies[max(current_player* -1,0)](current_player, game_state)\n",
    "        #Make the move\n",
    "        game_state = getUpdatedGameState(current_player, next_move, game_state)\n",
    "        #Log the move\n",
    "        game_states = np.vstack([game_states, game_state])\n",
    "        #Update turn count\n",
    "        turn_num += 1\n",
    "        #Check for a winner\n",
    "        winner = getWinner(game_state)\n",
    "    \n",
    "    turn_num = turn_num-1\n",
    "    #print(\"Player %s won in %s moves!\" % (winner, turn_num))\n",
    "    return (game_states, winner, turn_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePolicy(model=None,verbose=False):\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    def policy(current_player, game_state):\n",
    "        if current_player == -1:\n",
    "            game_state = game_state * -1\n",
    "        available_moves = np.where(game_state == 0)[0]\n",
    "        max_val = -99999\n",
    "        max_val_move = -1\n",
    "        for i,next_move in enumerate(available_moves):\n",
    "            next_state = np.copy(game_state)\n",
    "            next_state[next_move] = 1\n",
    "            move_val = model.predict(np.array([next_state]))[0]\n",
    "            if verbose:\n",
    "                print((next_move,move_val))\n",
    "            if max_val < move_val:\n",
    "                max_val = move_val\n",
    "                max_val_move = next_move\n",
    "            \n",
    "        return max_val_move\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(game_states, scores, reg=\"l1\", layers=[18,9,3]):\n",
    "    X_train = game_states\n",
    "    y_train = scores\n",
    "    \n",
    "    # Create neural net object\n",
    "    model = keras.Sequential()\n",
    "    if reg == \"l1_l2\":\n",
    "        # Adds a densely-connected layer with 18 units to the model:\n",
    "        model.add(layers.Dense(18, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(0.0001)))\n",
    "        # Add another (9):\n",
    "        model.add(layers.Dense(9, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(0.0001)))\n",
    "        # Add another (3):\n",
    "        model.add(layers.Dense(3, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(0.0001)))\n",
    "    elif reg == \"l2\":\n",
    "        # Adds a densely-connected layer with 18 units to the model:\n",
    "        model.add(layers.Dense(18, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\n",
    "        # Add another (9):\n",
    "        model.add(layers.Dense(9, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\n",
    "        # Add another (3):\n",
    "        model.add(layers.Dense(3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\n",
    "    else:\n",
    "        # Adds a densely-connected layer with 18 units to the model:\n",
    "        model.add(layers.Dense(18, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.0001)))\n",
    "        # Add another (9):\n",
    "        model.add(layers.Dense(9, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.0001)))\n",
    "        # Add another (3):\n",
    "        model.add(layers.Dense(3, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.0001)))\n",
    "    # Add a softmax layer with 1 unit:\n",
    "    model.add(layers.Dense(1, activation='tanh')) #tanh\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9,beta_2=0.999),\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n",
    "    #regr = MLPRegressor(solver='adam', early_stopping=True, #activation='logistic',\n",
    "                        #hidden_layer_sizes=(18, 9,3), random_state=42, max_iter=400)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    #regr.fit(X_train, y_train)\n",
    "    model.fit(X_train, y_train, epochs=800, batch_size=128, verbose=False)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #y_pred = model.predict(X_train, batch_size=32)\n",
    "\n",
    "    # The coefficients\n",
    "    #print('Coefficients: \\n', regr.coef_)\n",
    "    # The mean squared error\n",
    "    #print(\"Mean squared error: %.2f\"\n",
    "          #% mean_squared_error(y_train, y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    #print('Variance score: %.2f' % r2_score(y_train, y_pred))\n",
    "    #print(y_pred)\n",
    "    return (model, 0.5) #mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTrials(num_trials=10, model=None):\n",
    "    all_game_states = np.zeros((0,9))\n",
    "    all_scores = []\n",
    "    policy = makePolicy(model)\n",
    "    randPolicy = makePolicy()\n",
    "    for i in range(0,num_trials):\n",
    "        #Play game\n",
    "        game_states, winner, turn_num = playGame(policy,randPolicy)\n",
    "        if winner == 0:\n",
    "            #Skip ties\n",
    "            continue\n",
    "        #Normalize winner to 1\n",
    "        game_states = game_states*winner\n",
    "        winner = 1\n",
    "        #Just winner's turns\n",
    "        winner_states = game_states[::-2][::-1]\n",
    "        #Smooth reward backwards over time\n",
    "        winner_scores = decayScores(winner_states, 1)\n",
    "        #Loser's turns\n",
    "        loser_states = game_states[-2::-2][::-1]*-1\n",
    "        loser_scores = decayScores(loser_states, -1)\n",
    "        \n",
    "        game_states = np.vstack([winner_states, loser_states])\n",
    "        scores = np.append(winner_scores, loser_scores)\n",
    "        #Augment with reflections and inverses\n",
    "        game_states, scores = augmentData(game_states, scores)\n",
    "        #Add to data\n",
    "        all_game_states = np.vstack([all_game_states, game_states])\n",
    "        all_scores = np.append(all_scores, scores)\n",
    "\n",
    "    for i in range(0,num_trials):\n",
    "        #Play game\n",
    "        game_states, winner, turn_num = playGame(policy,policy)\n",
    "        if winner == 0:\n",
    "            #Skip ties\n",
    "            continue\n",
    "        #Normalize winner to 1\n",
    "        game_states = game_states*winner\n",
    "        winner = 1\n",
    "        #Just winner's turns\n",
    "        winner_states = game_states[::-2][::-1]\n",
    "        #Smooth reward backwards over time\n",
    "        winner_scores = decayScores(winner_states, 1)\n",
    "        #Loser's turns\n",
    "        loser_states = game_states[-2::-2][::-1]*-1\n",
    "        loser_scores = decayScores(loser_states, -1)\n",
    "        \n",
    "        game_states = np.vstack([winner_states, loser_states])\n",
    "        scores = np.append(winner_scores, loser_scores)\n",
    "        #Augment with reflections and inverses\n",
    "        game_states, scores = augmentData(game_states, scores)\n",
    "        #Add to data\n",
    "        all_game_states = np.vstack([all_game_states, game_states])\n",
    "        all_scores = np.append(all_scores, scores)\n",
    "    \n",
    "    regr, mse = trainModel(all_game_states, all_scores)\n",
    "    return regr, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGameData(num_trials=10, model1=None, model2=None):\n",
    "    all_game_states = np.zeros((0,9))\n",
    "    all_scores = []\n",
    "    policy1 = makePolicy(model1)\n",
    "    policy2 = makePolicy(model2)\n",
    "    for i in range(0,int(num_trials/2)):\n",
    "        #Play game\n",
    "        game_states, winner, turn_num = playGame(policy1,policy2)\n",
    "        if winner == 0:\n",
    "            #Skip ties\n",
    "            continue\n",
    "        #Normalize winner to 1\n",
    "        game_states = game_states*winner\n",
    "        winner = 1\n",
    "        #Just winner's turns\n",
    "        winner_states = game_states[::-2][::-1]\n",
    "        #Smooth reward backwards over time\n",
    "        winner_scores = decayScores(winner_states, 1)\n",
    "        #Loser's turns\n",
    "        loser_states = game_states[-2::-2][::-1]*-1\n",
    "        loser_scores = decayScores(loser_states, -1)\n",
    "        \n",
    "        game_states = np.vstack([winner_states, loser_states])\n",
    "        scores = np.append(winner_scores, loser_scores)\n",
    "        #Augment with reflections and inverses\n",
    "        game_states, scores = augmentData(game_states, scores)\n",
    "        #Add to data\n",
    "        all_game_states = np.vstack([all_game_states, game_states])\n",
    "        all_scores = np.append(all_scores, scores)\n",
    "\n",
    "    for i in range(0,int(num_trials/2)):\n",
    "        #Play game\n",
    "        game_states, winner, turn_num = playGame(policy2,policy1)\n",
    "        if winner == 0:\n",
    "            #Skip ties\n",
    "            continue\n",
    "        #Normalize winner to 1\n",
    "        game_states = game_states*winner\n",
    "        winner = 1\n",
    "        #Just winner's turns\n",
    "        winner_states = game_states[::-2][::-1]\n",
    "        #Smooth reward backwards over time\n",
    "        winner_scores = decayScores(winner_states, 1)\n",
    "        #Loser's turns\n",
    "        loser_states = game_states[-2::-2][::-1]*-1\n",
    "        loser_scores = decayScores(loser_states, -1)\n",
    "        \n",
    "        game_states = np.vstack([winner_states, loser_states])\n",
    "        scores = np.append(winner_scores, loser_scores)\n",
    "        #Augment with reflections and inverses\n",
    "        game_states, scores = augmentData(game_states, scores)\n",
    "        #Add to data\n",
    "        all_game_states = np.vstack([all_game_states, game_states])\n",
    "        all_scores = np.append(all_scores, scores)\n",
    "    \n",
    "    return (all_game_states, all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(num_trials=100, model1=None, model2=None, verbose=False):\n",
    "    winners = []\n",
    "    policy1 = makePolicy(model1)\n",
    "    policy2 = makePolicy(model2)\n",
    "\n",
    "    for i in range(0,int(num_trials/2)):\n",
    "        #Play game\n",
    "        game_states, winner, turn_num = playGame(policy1, policy2)\n",
    "        if winner == -1:\n",
    "            winner = 0\n",
    "        elif winner == 0:\n",
    "            winner = 1\n",
    "        winners.append(winner)\n",
    "    \n",
    "    for i in range(0,int(num_trials/2)):\n",
    "        #Play game\n",
    "        game_states, winner, turn_num = playGame(policy2, policy1)\n",
    "        if winner == 1:\n",
    "            winner = 0\n",
    "        elif winner == 0:\n",
    "            winner = -1\n",
    "        winners.append(winner * -1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(winners)\n",
    "    result = np.mean(winners)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "#Warm up with random vs random\n",
    "all_game_states, all_scores = getGameData(100)\n",
    "# Train an early model\n",
    "model1, mse = trainModel(all_game_states, all_scores)\n",
    "print(evaluateModel(20,model1=model1))\n",
    "# Make more data\n",
    "new_game_states, new_scores = getGameData(50,model1)\n",
    "# Stack\n",
    "all_game_states = np.vstack([all_game_states, new_game_states])\n",
    "all_scores = np.append(all_scores, new_scores)\n",
    "# Retrain\n",
    "model2, mse = trainModel(all_game_states, all_scores)\n",
    "print(evaluateModel(20,model1=model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L) 0.1334228515625 Play 50 games\n",
      "(L) 48.307918548583984 Train on 50 games\n",
      "0.82\n",
      "(L) 4.404000282287598 Evaluate on 50 games\n",
      "(L) 4.103925943374634 Play 50 more games\n",
      "(L) 105.73544454574585 Train on 100 games\n",
      "0.96\n",
      "(L) 4.506910800933838 Evaluate on 50 games\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def print_time(time_start,msg=\"\"):\n",
    "    time_diff = time.time()-time_start\n",
    "    print(f\"(L) {time_diff} {msg}\")\n",
    "    return time.time()\n",
    "\n",
    "time_start = time.time()\n",
    "#Warm up with random vs random\n",
    "all_game_states, all_scores = getGameData(50)\n",
    "time_start = print_time(time_start, \"Play 50 games\")\n",
    "# Train an early model\n",
    "model1, mse = trainModel(all_game_states, all_scores, reg=\"l1_l2\")\n",
    "time_start = print_time(time_start, \"Train on 50 games\")\n",
    "print(evaluateModel(50,model1=model1))\n",
    "time_start = print_time(time_start, \"Evaluate on 50 games\")\n",
    "# Make more data\n",
    "new_game_states, new_scores = getGameData(50,model1)\n",
    "time_start = print_time(time_start, \"Play 50 more games\")\n",
    "# Stack\n",
    "all_game_states = np.vstack([all_game_states, new_game_states])\n",
    "all_scores = np.append(all_scores, new_scores)\n",
    "time_start = time.time()\n",
    "# Retrain\n",
    "model2, mse = trainModel(all_game_states, all_scores, reg=\"l1_l2\")\n",
    "time_start = print_time(time_start, \"Train on 100 games\")\n",
    "print(evaluateModel(50,model1=model2))\n",
    "time_start = print_time(time_start, \"Evaluate on 50 games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L) 4.240751028060913 Play 50 more games\n",
      "(L) 148.84031915664673 Train on 150 games\n",
      "0.94\n",
      "(L) 8.706801414489746 Evaluate on 100 games\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "# Make more data\n",
    "new_game_states, new_scores = getGameData(50,model2)\n",
    "time_start = print_time(time_start, \"Play 50 more games\")\n",
    "# Stack\n",
    "all_game_states = np.vstack([all_game_states, new_game_states])\n",
    "all_scores = np.append(all_scores, new_scores)\n",
    "time_start = time.time()\n",
    "# Retrain\n",
    "model3, mse = trainModel(all_game_states, all_scores, reg=\"l1_l2\")\n",
    "time_start = print_time(time_start, \"Train on 150 games\")\n",
    "print(evaluateModel(100,model1=model3))\n",
    "time_start = print_time(time_start, \"Evaluate on 100 games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L) 162.75248312950134 Train L1\n",
      "0.93\n",
      "(L) 9.203482389450073 Evaluate L1 on 100 games\n",
      "(L) 177.13553428649902 Train L2\n",
      "0.9\n",
      "(L) 9.132536888122559 Evaluate L2 on 100 games\n",
      "(L) 163.85113143920898 Train ElasticNet\n",
      "0.99\n",
      "(L) 9.27957797050476 Evaluate ElasticNet on 100 games\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter testing\n",
    "time_start = time.time()\n",
    "# Retrain\n",
    "model_test, mse = trainModel(all_game_states, all_scores)\n",
    "time_start = print_time(time_start, \"Train L1\")\n",
    "print(evaluateModel(100,model1=model_test))\n",
    "time_start = print_time(time_start, \"Evaluate L1 on 100 games\")\n",
    "\n",
    "time_start = time.time()\n",
    "# Retrain\n",
    "model_test, mse = trainModel(all_game_states, all_scores, reg=\"l2\")\n",
    "time_start = print_time(time_start, \"Train L2\")\n",
    "print(evaluateModel(100,model1=model_test))\n",
    "time_start = print_time(time_start, \"Evaluate L2 on 100 games\")\n",
    "\n",
    "time_start = time.time()\n",
    "# Retrain\n",
    "model_test, mse = trainModel(all_game_states, all_scores, reg=\"l1_l2\")\n",
    "time_start = print_time(time_start, \"Train ElasticNet\")\n",
    "print(evaluateModel(100,model1=model_test))\n",
    "time_start = print_time(time_start, \"Evaluate ElasticNet on 100 games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993\n"
     ]
    }
   ],
   "source": [
    "print(evaluateModel(1000,model_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start game\n",
    "my_game_state = getNewGameState()\n",
    "# Create a log of game states\n",
    "my_game_states = np.zeros((0,9))\n",
    "#Train a model\n",
    "#regr, mse = doTrials()\n",
    "policy = makePolicy(model_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, array([-0.83492184], dtype=float32))\n",
      "(2, array([-0.9973592], dtype=float32))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1., -1.,  0.],\n",
       "        [-1.,  1.,  1.],\n",
       "        [-1.,  1., -1.]]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Opponent Move\n",
    "opponent_move = policy(-1, my_game_state)\n",
    "my_game_state[opponent_move] = -1\n",
    "my_game_states = np.vstack([my_game_states, my_game_state])\n",
    "prettyGameStates(np.array([my_game_state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.,  0.],\n",
       "        [-1.,  1.,  1.],\n",
       "        [-1.,  1., -1.]]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#My Move\n",
    "my_move = 7\n",
    "my_game_state[my_move] = 1\n",
    "my_game_states = np.vstack([my_game_states, my_game_state])\n",
    "prettyGameStates(np.array([my_game_state]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
